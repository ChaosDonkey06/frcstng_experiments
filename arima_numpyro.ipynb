{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2017-2019 Uber Technologies, Inc.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "import argparse\n",
    "from os.path import exists\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import pyro\n",
    "from pyro.contrib.timeseries import IndependentMaternGP, LinearlyCoupledMaternGP\n",
    "\n",
    "\n",
    "# download dataset from UCI archive\n",
    "def download_data():\n",
    "    if not exists(\"eeg.dat\"):\n",
    "        url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00264/EEG%20Eye%20State.arff\"\n",
    "        with open(\"eeg.dat\", \"wb\") as f:\n",
    "            f.write(urlopen(url).read())\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # download and pre-process EEG data if not in test mode\n",
    "    if not args.test:\n",
    "        download_data()\n",
    "        T_forecast = 349\n",
    "        data = np.loadtxt(\"eeg.dat\", delimiter=\",\", skiprows=19)\n",
    "        print(\"[raw data shape] {}\".format(data.shape))\n",
    "        data = torch.tensor(data[::20, :-1]).double()\n",
    "        print(\"[data shape after thinning] {}\".format(data.shape))\n",
    "    # in test mode (for continuous integration on github) so create fake data\n",
    "    else:\n",
    "        data = torch.randn(20, 3).double()\n",
    "        T_forecast = 10\n",
    "\n",
    "    T, obs_dim = data.shape\n",
    "    T_train = T - T_forecast\n",
    "\n",
    "    # standardize data\n",
    "    data_mean = data[0:T_train, :].mean(0)\n",
    "    data -= data_mean\n",
    "    data_std = data[0:T_train, :].std(0)\n",
    "    data /= data_std\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    # set up model\n",
    "    if args.model == \"imgp\":\n",
    "        gp = IndependentMaternGP(\n",
    "            nu=1.5, obs_dim=obs_dim, length_scale_init=1.5 * torch.ones(obs_dim)\n",
    "        ).double()\n",
    "    elif args.model == \"lcmgp\":\n",
    "        num_gps = 9\n",
    "        gp = LinearlyCoupledMaternGP(\n",
    "            nu=1.5,\n",
    "            obs_dim=obs_dim,\n",
    "            num_gps=num_gps,\n",
    "            length_scale_init=1.5 * torch.ones(num_gps),\n",
    "        ).double()\n",
    "\n",
    "    # set up optimizer\n",
    "    adam = torch.optim.Adam(\n",
    "        gp.parameters(),\n",
    "        lr=args.init_learning_rate,\n",
    "        betas=(args.beta1, 0.999),\n",
    "        amsgrad=True,\n",
    "    )\n",
    "    # we decay the learning rate over the course of training\n",
    "    gamma = (args.final_learning_rate / args.init_learning_rate) ** (\n",
    "        1.0 / args.num_steps\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(adam, gamma=gamma)\n",
    "\n",
    "    report_frequency = 10\n",
    "\n",
    "    # training loop\n",
    "    for step in range(args.num_steps):\n",
    "        loss = -gp.log_prob(data[0:T_train, :]).sum() / T_train\n",
    "        loss.backward()\n",
    "        adam.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if step % report_frequency == 0 or step == args.num_steps - 1:\n",
    "            print(\"[step %03d]  loss: %.3f\" % (step, loss.item()))\n",
    "\n",
    "    # plot predictions for three output dimensions\n",
    "    if args.plot:\n",
    "        assert not args.test\n",
    "\n",
    "        T_multistep = 49\n",
    "        T_onestep = T_forecast - T_multistep\n",
    "\n",
    "        # do rolling prediction\n",
    "        print(\"doing one-step-ahead forecasting...\")\n",
    "        onestep_means, onestep_stds = np.zeros((T_onestep, obs_dim)), np.zeros(\n",
    "            (T_onestep, obs_dim)\n",
    "        )\n",
    "        for t in range(T_onestep):\n",
    "            # predict one step into the future, conditioning on all previous data.\n",
    "            # note that each call to forecast() conditions on more data than the previous call\n",
    "            dts = torch.tensor([1.0]).double()\n",
    "            pred_dist = gp.forecast(data[0 : T_train + t, :], dts)\n",
    "            onestep_means[t, :] = pred_dist.loc.data.numpy()\n",
    "            if args.model == \"imgp\":\n",
    "                onestep_stds[t, :] = pred_dist.scale.data.numpy()\n",
    "            elif args.model == \"lcmgp\":\n",
    "                onestep_stds[t, :] = pred_dist.covariance_matrix.diagonal(\n",
    "                    dim1=-1, dim2=-2\n",
    "                ).data.numpy()\n",
    "\n",
    "        # do (non-rolling) multi-step forecasting\n",
    "        print(\"doing multi-step forecasting...\")\n",
    "        dts = (1 + torch.arange(T_multistep)).double()\n",
    "        pred_dist = gp.forecast(data[0 : T_train + T_onestep, :], dts)\n",
    "        multistep_means = pred_dist.loc.data.numpy()\n",
    "        if args.model == \"imgp\":\n",
    "            multistep_stds = pred_dist.scale.data.numpy()\n",
    "        elif args.model == \"lcmgp\":\n",
    "            multistep_stds = pred_dist.covariance_matrix.diagonal(\n",
    "                dim1=-1, dim2=-2\n",
    "            ).data.numpy()\n",
    "\n",
    "        import matplotlib\n",
    "\n",
    "        matplotlib.use(\"Agg\")  # noqa: E402\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        f, axes = plt.subplots(3, 1, figsize=(12, 8), sharex=True)\n",
    "        T = data.size(0)\n",
    "        to_seconds = 117.0 / T\n",
    "\n",
    "        for k, ax in enumerate(axes):\n",
    "            which = [0, 4, 10][k]\n",
    "\n",
    "            # plot raw data\n",
    "            ax.plot(\n",
    "                to_seconds * np.arange(T),\n",
    "                data[:, which],\n",
    "                \"ko\",\n",
    "                markersize=2,\n",
    "                label=\"Data\",\n",
    "            )\n",
    "\n",
    "            # plot mean predictions for one-step-ahead forecasts\n",
    "            ax.plot(\n",
    "                to_seconds * (T_train + np.arange(T_onestep)),\n",
    "                onestep_means[:, which],\n",
    "                ls=\"solid\",\n",
    "                color=\"b\",\n",
    "                label=\"One-step\",\n",
    "            )\n",
    "            # plot 90% confidence intervals for one-step-ahead forecasts\n",
    "            ax.fill_between(\n",
    "                to_seconds * (T_train + np.arange(T_onestep)),\n",
    "                onestep_means[:, which] - 1.645 * onestep_stds[:, which],\n",
    "                onestep_means[:, which] + 1.645 * onestep_stds[:, which],\n",
    "                color=\"b\",\n",
    "                alpha=0.20,\n",
    "            )\n",
    "\n",
    "            # plot mean predictions for multi-step-ahead forecasts\n",
    "            ax.plot(\n",
    "                to_seconds * (T_train + T_onestep + np.arange(T_multistep)),\n",
    "                multistep_means[:, which],\n",
    "                ls=\"solid\",\n",
    "                color=\"r\",\n",
    "                label=\"Multi-step\",\n",
    "            )\n",
    "            # plot 90% confidence intervals for multi-step-ahead forecasts\n",
    "            ax.fill_between(\n",
    "                to_seconds * (T_train + T_onestep + np.arange(T_multistep)),\n",
    "                multistep_means[:, which] - 1.645 * multistep_stds[:, which],\n",
    "                multistep_means[:, which] + 1.645 * multistep_stds[:, which],\n",
    "                color=\"r\",\n",
    "                alpha=0.20,\n",
    "            )\n",
    "\n",
    "            ax.set_ylabel(\"$y_{%d}$\" % (which + 1), fontsize=20)\n",
    "            ax.tick_params(axis=\"both\", which=\"major\", labelsize=14)\n",
    "            if k == 1:\n",
    "                ax.legend(loc=\"upper left\", fontsize=16)\n",
    "\n",
    "        plt.tight_layout(pad=0.7)\n",
    "        plt.savefig(\"eeg.{}.pdf\".format(args.model))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
